# Fusion Ablation Study Configuration
# Compare different fusion mechanisms for multimodal emotion recognition

wandb_project: "Multimodal_Fusion_Ablation"

template_config: &template
  # Training hyperparameters (optimal)
  learning_rate: 9e-3
  weight_decay: 5e-6
  num_epochs: 30
  batch_size: 64
  dropout: 0.1

  # Dataset settings
  train_dataset: "IEMO"
  evaluation_mode: "cross_corpus"

  # Multimodal settings
  modality: "both"
  audio_dim: 768
  text_model_name: "bert-base-uncased"
  freeze_text_encoder: true
  text_max_length: 128

  # Model architecture
  hidden_dim: 1024
  num_classes: 4

  # Fusion settings (will be overridden per experiment)
  fusion_hidden_dim: 512
  num_attention_heads: 8

  # Class weights
  class_weights:
    neutral: 1.0
    happy: 1.0
    sad: 1.0
    anger: 1.0

  # Optimal training strategy
  use_curriculum_learning: true
  curriculum_epochs: 15
  curriculum_pacing: "sqrt"
  curriculum_type: "difficulty"
  difficulty_method: "euclidean_distance"
  use_difficulty_scaling: true
  use_speaker_disentanglement: true

  # Expected VAD values
  expected_vad:
    0: [3.0, 2.5, 3.0]  # neutral
    1: [4.0, 3.8, 3.8]  # happy
    2: [1.8, 2.2, 2.0]  # sad
    3: [1.8, 4.2, 4.0]  # anger

experiments:
  # Main fusion mechanisms
  - <<: *template
    name: "Cross-Attention Fusion"
    fusion_type: "cross_attention"
    fusion_hidden_dim: 512
    num_attention_heads: 8

  - <<: *template
    name: "Simple Concat Fusion"
    fusion_type: "concat"
    fusion_hidden_dim: 512

  - <<: *template
    name: "Gated Fusion"
    fusion_type: "gated"
    fusion_hidden_dim: 512

  - <<: *template
    name: "Adaptive Fusion"
    fusion_type: "adaptive"
    fusion_hidden_dim: 512

  # Cross-attention with different hidden dimensions
  - <<: *template
    name: "Cross-Attention 256"
    fusion_type: "cross_attention"
    fusion_hidden_dim: 256
    num_attention_heads: 4

  - <<: *template
    name: "Cross-Attention 1024"
    fusion_type: "cross_attention"
    fusion_hidden_dim: 1024
    num_attention_heads: 16

  # Cross-attention with different attention heads
  - <<: *template
    name: "Cross-Attention 4-heads"
    fusion_type: "cross_attention"
    fusion_hidden_dim: 512
    num_attention_heads: 4

  - <<: *template
    name: "Cross-Attention 16-heads"
    fusion_type: "cross_attention"
    fusion_hidden_dim: 512
    num_attention_heads: 16
